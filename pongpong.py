# -*- coding: utf-8 -*-
"""PONGPONG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w8z2iROGjQvVqcQuVUoKtOrD5cyM4G3d
"""

!git clone https://github.com/PreethamPasala/Artificial-Inteligence-PJ-.git
!git clone https://github.com/openai/gym.git
!git clone https://github.com/openai/atari-py.git

!pip install tensorflow==1.14

!pip install pip

!pip install gym[atari]
!pip install openai
!pip install pyvirtualdisplay
!pip install piglet
!apt-get install python-opengl -y
!apt install xvfb -y

!pip install keras
!pip install atari-py

!pip install gym-retro

!python -m retro.import /content/drive/MyDrive

# Commented out IPython magic to ensure Python compatibility.
# %cd Artificial-Inteligence-PJ-/ 
import numpy as np
import gym
from policyGradientNetwork import Network

# variables
hidden_layer_size=200
learning_rate=0.0006
checkpointNumber=10
load_checkpoint=True
discount_factor=0.99
render=True
batch_size = 1

# preprocess
def preprocess(I):
    """ prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector """
    I = I[35:195]  # crop
    I = I[::2, ::2, 0]  # downsample by factor of 2
    I[I == 144] = 0  # erase background (background type 1)
    I[I == 109] = 0  # erase background (background type 2)
    I[I != 0] = 1  # everything else (paddles, ball) just set to 1
    return I.astype(np.float).ravel()

# compute discounted rewards
def discount_rewards(rewards, discount_factor):
    discounted_rewards = np.zeros_like(rewards)
    for t in range(len(rewards)):
        discounted_reward_sum = 0
        discount = 1
        for k in range(t, len(rewards)):
            discounted_reward_sum += rewards[k] * discount
            discount *= discount_factor
            if rewards[k] != 0:
                # Don't count rewards from subsequent rounds
                break
        discounted_rewards[t] = discounted_reward_sum
    return discounted_rewards

# Commented out IPython magic to ensure Python compatibility.
import gym
import atari_py
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(40) #error only
import tensorflow as tf
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import glob
import io
import base64
from IPython.display import HTML

from IPython import display as ipythondisplay

"""
Utility functions to enable video recording of gym environment and displaying it
To enable video, just do "env = wrap_env(env)""
"""

def show_video():
  mp4list = glob.glob('video/*.mp4')
  if len(mp4list) > 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
  else: 
    print("Could not find video")
    

def wrap_env(env):
  env = Monitor(env, './video', force=True)
  return env

from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()

# Commented out IPython magic to ensure Python compatibility.
# This code creates a virtual display to draw game images on. 
# If you are running locally, just ignore it
import os
if type(os.environ.get("DISPLAY")) is not str or len(os.environ.get("DISPLAY"))==0:
    !bash ../xvfb start
#     %env DISPLAY=:1

!pip install atari-py

!python -m atari_py.import_roms /content/drive/MyDrive

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

# use gym environment: Pong-v0
# initialization of environment
environment = gym.make("Pong-v0")
observation = environment.reset()

# Action values to send to gym environment to move paddle up/down
UP_ACTION = 2
DOWN_ACTION = 3
action_dict = {DOWN_ACTION: 0, UP_ACTION: 1}

policyTFNetwork = Network(hidden_layer_size, learning_rate, checkpoints_dir='checkpoints')
if load_checkpoint:
    policyTFNetwork.load_checkpoint()

batch_state_action_reward_tuples = []
smoothed_reward = None
episode_number = 1

while True:
    done = False
    total_reward = 0
    round_number = 1

    last_observation = environment.reset()
    last_observation = preprocess(last_observation)
    action = environment.action_space.sample()

    # step the environment and get new measurements
    observation, reward, done, info = environment.step(action)
    observation = preprocess(observation)
    number_steps = 1

    environment = wrap_env(environment)
    environment.reset()

    while True:
      if render:
        environment.render()
            
        observation_delta = observation - last_observation
        last_observation = observation
        update_probability = policyTFNetwork.forward_pass(observation_delta)[0]
        if np.random.uniform() < update_probability:
            action = UP_ACTION
        else:
            action = DOWN_ACTION

        # step the environment and get measurements
        observation, reward, done, info = environment.step(action)
        if done:
          break
        observation = preprocess(observation)
        total_reward += reward
        number_steps += 1

        # add action, observation_delta and reward
        tup = (observation_delta, action_dict[action], reward)
        batch_state_action_reward_tuples.append(tup)

        if reward != 0:
            print('Episode no: %d,  Game round finished, Reward: %f, Total Reward: %f' % (episode_number, reward, total_reward))
            round_number += 1
            n_steps = 0
            
    environment.close()

    # exponentially smoothed version of reward
    if smoothed_reward is None:
        smoothed_reward = total_reward
    else:
        smoothed_reward = smoothed_reward * 0.99 + total_reward * 0.01
    print("Total Reward was %f; running mean reward is %f" % (total_reward, smoothed_reward))

    # perform rmsprop parameter update every batch_size episodes
    if episode_number % batch_size == 0:
        states, actions, rewards = zip(*batch_state_action_reward_tuples)
        rewards = discount_rewards(rewards, discount_factor)
        rewards = rewards - np.mean(rewards)
        rewards = rewards / np.std(rewards)
        batch_state_action_reward_tuples = list(zip(states, actions, rewards))
        policyTFNetwork.train(batch_state_action_reward_tuples)
        batch_state_action_reward_tuples = []

    # save episodes for checkpoint
    if episode_number % checkpointNumber == 0:
        policyTFNetwork.save_checkpoint()

    episode_number += 1
    show_video()



